{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1eNqGzMTgm+aOKtywMySI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhihong1224/RNN_demo/blob/master/Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OgXSgWy5WSS",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9YPr5Ob4xuT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "9baf354f-5335-4c95-ada4-88bb84a27810"
      },
      "source": [
        "!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n",
            "To: /content/data.zip\n",
            "45.1MB [00:00, 67.7MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: training_label.txt      \n",
            "  inflating: testing_data.txt        \n",
            "  inflating: training_nolabel.txt    \n",
            "data.zip     testing_data.txt\t training_nolabel.txt\n",
            "sample_data  training_label.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogx0o8i95BHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sumTskqh5TYL",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opjs8WiN5KGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def loading_training_data(path='training_label.txt'):\n",
        "  if 'training_label' in path:\n",
        "    with open(path,'r') as f:\n",
        "      lines=f.readlines()\n",
        "      lines=[line.strip('\\n').split(' ') for line in lines]\n",
        "    x=[line[2:] for line in lines]\n",
        "    y=[line[0] for line in lines]\n",
        "    return x,y\n",
        "  else:\n",
        "    with open(path,'r') as f:\n",
        "      lines=f.readlines()\n",
        "      x=[line.strip('\\n').split(' ') for line in lines]\n",
        "    return x\n",
        "def load_testing_data(path='testing_data'):\n",
        "  with open(path,'r') as f:\n",
        "    lines=f.readlines()\n",
        "    X=[''.join(line.strip('\\n').split(',')[1:]).strip() for line in lines]\n",
        "    X=[sen.split(' ') for sen in X]\n",
        "  return X\n",
        "def evaluation(outputs,labels):\n",
        "  outputs[outputs>=0.5]=1\n",
        "  outputs[outputs<0.5]=0\n",
        "  correct=torch.sum(torch.eq(outputs,labels)).item()\n",
        "  return correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkymM8Dr81WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data,train_label=loading_training_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLc8Et2m864m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2b2b01cd-acd5-4ab7-c933-5caf395ae340"
      },
      "source": [
        "print(len(train_data),len(train_data[100]),train_data[2])\n",
        "print(len(train_label),train_label[100])\n",
        "print(max([len(line) for line in train_data]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200000 16 ['i', 'wish', 'i', 'could', 'go', 'and', 'see', 'duffy', 'when', 'she', 'comes', 'to', 'mamaia', 'romania', '.']\n",
            "200000 0\n",
            "39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO7PRNlv7Zia",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Zp_fyvj6W3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocess():\n",
        "  def __init__(self,sentences):\n",
        "    # sentences:train_data list of reviews\n",
        "    self.sentences=[[word.lower() for word in line] for line in sentences]\n",
        "    # to get word_to_idx,idx_to_word,vocab_size\n",
        "    words=[word for line in self.sentences for word in line]\n",
        "    set_words=set(words)\n",
        "    self.idx_to_word=[word for word in set_words]\n",
        "    self.idx_to_word.append('<pad>')\n",
        "    self.word_to_idx={word:idx for idx,word in enumerate(self.idx_to_word)}\n",
        "    self.word_to_idx['<pad>']=len(self.word_to_idx)\n",
        "    self.vocab_size=len(self.idx_to_word)\n",
        "  def get_corpus(self,seq_len,device=device):\n",
        "    # 获取传入模型的数据，(batch_size,seq_len)\n",
        "    results=[]\n",
        "    L=len(self.sentences)\n",
        "    for i in range(L):\n",
        "      line=self.sentences[i]\n",
        "      if len(line)>seq_len:\n",
        "        temp=[self.word_to_idx[word] for word in line[:seq_len]]\n",
        "      else:\n",
        "        temp=[self.word_to_idx[word] for word in line]+[self.word_to_idx['<pad>']]*(seq_len-len(line))\n",
        "      results.append(temp)\n",
        "    return torch.tensor(results)\n",
        "  def labels_to_tensor(self,y):\n",
        "    # y:list of labels\n",
        "    y=[int(label) for label in y]\n",
        "    return torch.tensor(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rji17xbOGvS4",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pi0QvmF_hDm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8d049a9d-bf01-4796-f2eb-d936c4135b62"
      },
      "source": [
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "train_data,train_y=loading_training_data()\n",
        "preprocess=Preprocess(train_data)\n",
        "vocab_size=preprocess.vocab_size\n",
        "print(len(train_y))\n",
        "\n",
        "seq_len=20\n",
        "train_all_corpus=preprocess.get_corpus(seq_len)\n",
        "train_all_labels=preprocess.labels_to_tensor(train_y)\n",
        "print(train_all_labels.shape)\n",
        "\n",
        "train_all=len(train_data)\n",
        "portion=0.2\n",
        "valid_num=int(train_all*portion)\n",
        "\n",
        "valid_corpus=train_all_corpus[:valid_num,:]\n",
        "valid_labels=train_all_labels[:valid_num]\n",
        "valid_dataset=TensorDataset(valid_corpus,valid_labels)\n",
        "\n",
        "train_corpus=train_all_corpus[valid_num:,:]\n",
        "train_labels=train_all_labels[valid_num:]\n",
        "train_dataset=TensorDataset(train_corpus,train_labels)\n",
        "\n",
        "batch_size=400\n",
        "\n",
        "train_iter=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "valid_iter=DataLoader(valid_dataset,batch_size=batch_size,shuffle=True,num_workers=4)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200000\n",
            "torch.Size([200000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3B585RvjuvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0fa49e6-ecf6-4ac4-91e8-d55cbe70f4b7"
      },
      "source": [
        "for X,Y in train_iter:\n",
        "  print(X.shape,Y.shape)\n",
        "  break"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400, 20]) torch.Size([400])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uKw_6TVI4jb",
        "colab_type": "text"
      },
      "source": [
        "# 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WBzVjMCI5se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sentiment(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_size,hidden_size):\n",
        "    super(Sentiment,self).__init__()\n",
        "    self.embed=nn.Embedding(vocab_size,embed_size)\n",
        "    self.lstm=nn.LSTM(embed_size,hidden_size,num_layers=2,batch_first=True)\n",
        "    self.dropout=nn.Dropout(0.5)\n",
        "    self.fc=nn.Linear(hidden_size,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "  def forward(self,x):\n",
        "    # x:(batch_size,seq_len)\n",
        "    embed=self.embed(x)   # (batch_size,seq_len,embed_size)\n",
        "    out,_=self.lstm(embed,None)  # (batch_size,num_layers,hidden_size)\n",
        "    out=out[:,-1,:]   # (batch_size,hidden_size)\n",
        "    out=self.dropout(out) # (batch_size,hidden_size)\n",
        "    out=self.sigmoid(self.fc(out)) # (batch_size,1)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzESj5RbN6aG",
        "colab_type": "text"
      },
      "source": [
        "# 训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho8g1F5HN55Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,num_epochs,lr,train_iter,valid_iter,print_every=100):\n",
        "  model=model.to(device)\n",
        "  criterion=nn.BCELoss()\n",
        "  optimizer=optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss,train_acc,n=0.0,0.0,0\n",
        "    for i,(X,Y) in enumerate(train_iter):\n",
        "      X=X.to(device)\n",
        "      Y=Y.to(device)\n",
        "      y_pred=model(X).squeeze()\n",
        "      loss=criterion(y_pred,Y.float())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss+=loss.item()\n",
        "      train_acc+=evaluation(y_pred,Y.float())\n",
        "      n+=Y.shape[0]\n",
        "      if i%print_every==0:\n",
        "        print('[ Epoch{}: {}/{}] loss:{:.3f} acc:{:.3f}'.\\\n",
        "            format(epoch+1,i,len(train_iter),loss.item(),train_acc/n))\n",
        "    valid_acc=evaluate_acc(model,valid_iter)\n",
        "    print('\\nTrain | Loss:{:.5f} Train_Acc:{:.3f} Valid_Acc:{:.3f}'\\\n",
        "          .format(train_loss/len(train_iter),train_acc/n,valid_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKAX3vgXRefg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_acc(model,valid_iter):\n",
        "  model.eval()\n",
        "  acc,n=0.0,0\n",
        "  with torch.no_grad():\n",
        "    for X,Y in valid_iter:\n",
        "      X=X.to(device)\n",
        "      Y=Y.to(device)\n",
        "      y_pred=model(X).squeeze()\n",
        "      acc+=evaluation(y_pred,Y)\n",
        "      n+=Y.shape[0]\n",
        "  model.train()\n",
        "  return acc/n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewubt7H3ijOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16abec54-f6e8-4bb6-fbcb-ffd448c069dc"
      },
      "source": [
        "num_epochs,lr=10,0.001\n",
        "model=Sentiment(vocab_size,1024,256)\n",
        "train(model,num_epochs,lr,train_iter,valid_iter)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Epoch1: 0/400] loss:0.692 acc:0.512\n",
            "[ Epoch1: 100/400] loss:0.494 acc:0.695\n",
            "[ Epoch1: 200/400] loss:0.512 acc:0.729\n",
            "[ Epoch1: 300/400] loss:0.451 acc:0.745\n",
            "\n",
            "Train | Loss:0.49838 Train_Acc:0.755 Valid_Acc:0.790\n",
            "[ Epoch2: 0/400] loss:0.383 acc:0.848\n",
            "[ Epoch2: 100/400] loss:0.430 acc:0.827\n",
            "[ Epoch2: 200/400] loss:0.374 acc:0.826\n",
            "[ Epoch2: 300/400] loss:0.379 acc:0.826\n",
            "\n",
            "Train | Loss:0.38884 Train_Acc:0.825 Valid_Acc:0.797\n",
            "[ Epoch3: 0/400] loss:0.280 acc:0.902\n",
            "[ Epoch3: 100/400] loss:0.316 acc:0.879\n",
            "[ Epoch3: 200/400] loss:0.315 acc:0.877\n",
            "[ Epoch3: 300/400] loss:0.307 acc:0.876\n",
            "\n",
            "Train | Loss:0.29357 Train_Acc:0.875 Valid_Acc:0.790\n",
            "[ Epoch4: 0/400] loss:0.211 acc:0.920\n",
            "[ Epoch4: 100/400] loss:0.154 acc:0.928\n",
            "[ Epoch4: 200/400] loss:0.195 acc:0.925\n",
            "[ Epoch4: 300/400] loss:0.185 acc:0.922\n",
            "\n",
            "Train | Loss:0.19810 Train_Acc:0.921 Valid_Acc:0.782\n",
            "[ Epoch5: 0/400] loss:0.138 acc:0.960\n",
            "[ Epoch5: 100/400] loss:0.111 acc:0.962\n",
            "[ Epoch5: 200/400] loss:0.146 acc:0.959\n",
            "[ Epoch5: 300/400] loss:0.140 acc:0.956\n",
            "\n",
            "Train | Loss:0.12217 Train_Acc:0.954 Valid_Acc:0.776\n",
            "[ Epoch6: 0/400] loss:0.066 acc:0.975\n",
            "[ Epoch6: 100/400] loss:0.055 acc:0.977\n",
            "[ Epoch6: 200/400] loss:0.053 acc:0.975\n",
            "[ Epoch6: 300/400] loss:0.066 acc:0.974\n",
            "\n",
            "Train | Loss:0.07734 Train_Acc:0.972 Valid_Acc:0.769\n",
            "[ Epoch7: 0/400] loss:0.038 acc:0.993\n",
            "[ Epoch7: 100/400] loss:0.035 acc:0.985\n",
            "[ Epoch7: 200/400] loss:0.051 acc:0.984\n",
            "[ Epoch7: 300/400] loss:0.056 acc:0.982\n",
            "\n",
            "Train | Loss:0.05507 Train_Acc:0.981 Valid_Acc:0.771\n",
            "[ Epoch8: 0/400] loss:0.016 acc:0.995\n",
            "[ Epoch8: 100/400] loss:0.020 acc:0.990\n",
            "[ Epoch8: 200/400] loss:0.054 acc:0.989\n",
            "[ Epoch8: 300/400] loss:0.038 acc:0.988\n",
            "\n",
            "Train | Loss:0.03842 Train_Acc:0.987 Valid_Acc:0.766\n",
            "[ Epoch9: 0/400] loss:0.027 acc:0.993\n",
            "[ Epoch9: 100/400] loss:0.019 acc:0.992\n",
            "[ Epoch9: 200/400] loss:0.041 acc:0.991\n",
            "[ Epoch9: 300/400] loss:0.055 acc:0.990\n",
            "\n",
            "Train | Loss:0.03058 Train_Acc:0.990 Valid_Acc:0.771\n",
            "[ Epoch10: 0/400] loss:0.023 acc:0.993\n",
            "[ Epoch10: 100/400] loss:0.018 acc:0.992\n",
            "[ Epoch10: 200/400] loss:0.023 acc:0.992\n",
            "[ Epoch10: 300/400] loss:0.030 acc:0.991\n",
            "\n",
            "Train | Loss:0.02772 Train_Acc:0.991 Valid_Acc:0.768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wICFRgdVjC5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}